{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "816f17a7-ab69-4f55-b2c6-dd979039a0e3",
   "metadata": {},
   "source": [
    "Objective: \n",
    "Write a program to implement a multi-layer perceptron (MLP) network with one hidden layer using numpy in Python. Demonstrate that it can learn the XOR Boolean function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108066fa-0997-4150-98cc-1444107d90ab",
   "metadata": {},
   "source": [
    "Description of Model:\n",
    "A Multi-Layer Perceptron (MLP) is a type of artificial neural network (ANN) designed to model complex patterns and relationships in data. Unlike a simple perceptron, which can only solve linearly separable problems, an MLP can learn non-linear functions using multiple layers of neurons.\n",
    "\n",
    "Architecture of MLP:-\n",
    "\n",
    "Input Layer:\n",
    "Takes in raw features (e.g., two input values for XOR).\n",
    "\n",
    "Hidden Layer(s):\n",
    "Contains one or more layers of neurons that apply weighted sums and activation functions to learn complex relationships.\n",
    "In this model, we have one hidden layer with two neurons.\n",
    "\n",
    "Output Layer:\n",
    "Produces the final prediction (binary classification in this case).\n",
    "This model has a single output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9658a7c6-77a8-46b9-954b-ef1dfd8176f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 1 1 0]\n",
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def step_function(x):\n",
    "    # Step activation function:\n",
    "    return np.where(x >= 0, 1, 0)\n",
    "\n",
    "def train_mlp(X, y, hidden_neurons=4, epochs=10, learning_rate=0.1):\n",
    "    input_neurons = X.shape[1]\n",
    "    output_neurons = 1\n",
    "    \n",
    "    # Initialize weights and biases randomly\n",
    "    W1 = np.random.uniform(-1, 1, (hidden_neurons, input_neurons))\n",
    "    b1 = np.random.uniform(-1, 1, (hidden_neurons, 1)) \n",
    "    W2 = np.random.uniform(-1, 1, (output_neurons, hidden_neurons))\n",
    "    b2 = np.random.uniform(-1, 1, (output_neurons, 1))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(X.shape[0]):\n",
    "            x_sample = X[i].reshape(-1, 1)\n",
    "            y_sample = y[i]\n",
    "            \n",
    "            # Forward pass\n",
    "            hidden_input = np.dot(W1, x_sample) + b1\n",
    "            hidden_output = step_function(hidden_input)\n",
    "            final_input = np.dot(W2, hidden_output) + b2\n",
    "            final_output = step_function(final_input)\n",
    "\n",
    "            error = y_sample- final_output\n",
    "            # Random weight update rule\n",
    "            if final_output != y_sample:\n",
    "                error_l1 = np.dot(W2.T,error)\n",
    "                W2 += learning_rate * (error) * hidden_output.T\n",
    "                b2 += learning_rate * (error)\n",
    "                W1 += learning_rate * error_l1 * x_sample.T\n",
    "                b1 += learning_rate * (error)\n",
    "    \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def predict(X, W1, b1, W2, b2):\n",
    "    hidden_input = np.dot(W1, X.T) + b1\n",
    "    hidden_output = step_function(hidden_input)\n",
    "    final_input = np.dot(W2, hidden_output) + b2\n",
    "    final_output = step_function(final_input)\n",
    "    return final_output.T\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred) * 100\n",
    "\n",
    "# XOR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]]) # output of XOR gate\n",
    "\n",
    "# Train the MLP\n",
    "W1, b1, W2, b2 = train_mlp(X, y, hidden_neurons=4)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = predict(X, W1, b1, W2, b2)\n",
    "\n",
    "# Compute accuracy\n",
    "acc = accuracy(y, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Predictions:\", y_pred.flatten())\n",
    "print(f\"Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79031c0-236e-43a7-9533-e3dc6476b6fd",
   "metadata": {},
   "source": [
    "Description of code:\n",
    "This Python code implements a Multi-Layer Perceptron (MLP) using NumPy to classify the XOR function.\n",
    "\n",
    "Step Function: Used as an activation function.\n",
    "Training (train_mlp): Initializes weights, performs forward pass, updates weights if the prediction is incorrect.\n",
    "Prediction (predict): Computes activations for hidden and output layers.\n",
    "Accuracy Calculation (accuracy): Compares predicted and actual values.\n",
    "Execution: Trains the model on the XOR dataset, makes predictions, and prints accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fe30e6-31e8-436a-890e-884e9a3fbed2",
   "metadata": {},
   "source": [
    "Comments:\n",
    "A single-layer perceptron fails to solve XOR because XOR is not linearly separable\n",
    "MLP overcomes this by introducing a hidden layer, allowing it to learn non-linear decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a049c1b2-e2fb-4bc8-a784-d16ddfa65e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
