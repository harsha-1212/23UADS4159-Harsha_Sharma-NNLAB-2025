{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "816f17a7-ab69-4f55-b2c6-dd979039a0e3",
   "metadata": {},
   "source": [
    "Objective: \n",
    "Write a program to implement a multi-layer perceptron (MLP) network with one hidden layer using numpy in Python. Demonstrate that it can learn the XOR Boolean function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108066fa-0997-4150-98cc-1444107d90ab",
   "metadata": {},
   "source": [
    "Description of Model:\n",
    "A Multi-Layer Perceptron (MLP) is a type of artificial neural network (ANN) designed to model complex patterns and relationships in data. Unlike a simple perceptron, which can only solve linearly separable problems, an MLP can learn non-linear functions using multiple layers of neurons.\n",
    "\n",
    "Architecture of MLP:-\n",
    "\n",
    "Input Layer:\n",
    "Takes in raw features (e.g., two input values for XOR).\n",
    "\n",
    "Hidden Layer(s):\n",
    "Contains one or more layers of neurons that apply weighted sums and activation functions to learn complex relationships.\n",
    "In this model, we have one hidden layer with two neurons.\n",
    "\n",
    "Output Layer:\n",
    "Produces the final prediction (binary classification in this case).\n",
    "This model has a single output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9658a7c6-77a8-46b9-954b-ef1dfd8176f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Total Error: 1\n",
      "Epoch 1000, Total Error: 2\n",
      "Epoch 2000, Total Error: 2\n",
      "Epoch 3000, Total Error: 2\n",
      "Epoch 4000, Total Error: 2\n",
      "Epoch 5000, Total Error: 2\n",
      "Epoch 6000, Total Error: 2\n",
      "Epoch 7000, Total Error: 2\n",
      "Epoch 8000, Total Error: 2\n",
      "Epoch 9000, Total Error: 2\n",
      "\n",
      "Testing the trained MLP:\n",
      "Input: [0 0], Predicted Output: [0]\n",
      "Input: [0 1], Predicted Output: [0]\n",
      "Input: [1 0], Predicted Output: [0]\n",
      "Input: [1 1], Predicted Output: [0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step activation function\n",
    "def step_function(x):\n",
    "    return np.where(x >= 0, 1, 0)\n",
    "\n",
    "# XOR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Initialize weights and biases\n",
    "input_size = 2\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.random.randn(hidden_size)\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.random.randn(output_size)\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_error = 0\n",
    "    for i in range(len(X)):\n",
    "        # Forward pass\n",
    "        hidden_input = np.dot(X[i], W1) + b1\n",
    "        hidden_output = step_function(hidden_input)\n",
    "        final_input = np.dot(hidden_output, W2) + b2\n",
    "        final_output = step_function(final_input)\n",
    "        \n",
    "        # Compute error\n",
    "        error = y[i] - final_output\n",
    "        total_error += abs(error)\n",
    "        \n",
    "        # Backpropagation (Perceptron learning rule)\n",
    "        W2 += learning_rate * error * hidden_output.reshape(-1, 1)\n",
    "        b2 += learning_rate * error\n",
    "        W1 += learning_rate * error * np.outer(X[i], hidden_output * W2.T)\n",
    "        b1 += learning_rate * error * W2.T.flatten()\n",
    "    \n",
    "    # Print error at each epoch\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Total Error: {total_error.sum()}\")\n",
    "\n",
    "# Testing the trained MLP\n",
    "print(\"\\nTesting the trained MLP:\")\n",
    "for i in range(len(X)):\n",
    "    hidden_input = np.dot(X[i], W1) + b1\n",
    "    hidden_output = step_function(hidden_input)\n",
    "    final_input = np.dot(hidden_output, W2) + b2\n",
    "    final_output = step_function(final_input)\n",
    "    print(f\"Input: {X[i]}, Predicted Output: {final_output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79031c0-236e-43a7-9533-e3dc6476b6fd",
   "metadata": {},
   "source": [
    "Description of code:\n",
    "This code implements a simple Multi-Layer Perceptron (MLP) from scratch to solve the XOR problem, which is not linearly separable.\n",
    "The model consists of three layers:\n",
    "\n",
    "Input Layer:- Takes two binary inputs.\n",
    "Hidden Layer:- Contains two neurons to learn non-linear features.\n",
    "Output Layer:- Produces a single binary output (0 or 1).\n",
    "\n",
    "The training process includes forward propagation, where the input passes through the network, and a step activation function determines neuron activations. The model is trained using a Perceptron learning rule instead of standard backpropagation. The weights and biases are updated iteratively to minimize errors.\n",
    "\n",
    "After 10,000 training epochs, the model is tested on XOR inputs to verify its predictions. The implementation demonstrates how an MLP can learn complex patterns that a single-layer perceptron cannot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fe30e6-31e8-436a-890e-884e9a3fbed2",
   "metadata": {},
   "source": [
    "Comments:\n",
    "A single-layer perceptron fails to solve XOR because XOR is not linearly separable\n",
    "MLP overcomes this by introducing a hidden layer, allowing it to learn non-linear decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a049c1b2-e2fb-4bc8-a784-d16ddfa65e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
