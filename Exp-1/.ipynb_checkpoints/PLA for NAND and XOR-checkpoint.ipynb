{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8ea7aea-d9b9-4bed-a31b-66b1e914049a",
   "metadata": {},
   "source": [
    "Objective:- \n",
    "Write a program to implement the Perceptron Learning Algorithm using numpy in Python. Evaluate performance of a single perceptron for NAND and XOR truth tables as input dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422a573b-de70-4cd1-a028-ef1f58dc9b3d",
   "metadata": {},
   "source": [
    "Description of model:-\n",
    "A perceptron is a binary classifier that separates data with a linear decision boundary. It is trained using Perceptron Learning Rule, which updates the weights iteratively to reduce misclassification.\n",
    "We apply activation function i.e. step function in the model and train the model by learing algorithmn of perceptron. We use the dataset of NAND ans XOR and apply the perceptron learning rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3389d1f-7b58-410e-b917-9ac7ef8f31c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAND Perceptron Accuracy: 100.0%\n",
      "XOR Perceptron Accuracy: 25.0%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, learning_rate=0.1, epochs=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.weights = np.random.randn(input_size + 1)  # +1 for bias term\n",
    "\n",
    "    def activation(self, x):\n",
    "        return 1 if x >= 0 else 0\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.insert(x, 0, 1)  # Add bias term\n",
    "        return self.activation(np.dot(self.weights, x))\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for _ in range(self.epochs):\n",
    "            for xi, target in zip(X, y):\n",
    "                xi = np.insert(xi, 0, 1)  # Add bias term\n",
    "                prediction = self.activation(np.dot(self.weights, xi))\n",
    "                error = target - prediction\n",
    "                self.weights += self.learning_rate * error * xi\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        correct = sum(self.predict(xi) == target for xi, target in zip(X, y))\n",
    "        accuracy = correct / len(y) * 100\n",
    "        return accuracy\n",
    "\n",
    "# NAND Truth Table\n",
    "X_NAND = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_NAND = np.array([1, 1, 1, 0])\n",
    "\n",
    "# XOR Truth Table\n",
    "X_XOR = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_XOR = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Train and evaluate perceptron for NAND\n",
    "perceptron_nand = Perceptron(input_size=2)\n",
    "perceptron_nand.train(X_NAND, y_NAND)\n",
    "nand_accuracy = perceptron_nand.evaluate(X_NAND, y_NAND)\n",
    "print(f\"NAND Perceptron Accuracy: {nand_accuracy}%\")\n",
    "\n",
    "# Train and evaluate perceptron for XOR\n",
    "perceptron_xor = Perceptron(input_size=2)\n",
    "perceptron_xor.train(X_XOR, y_XOR)\n",
    "xor_accuracy = perceptron_xor.evaluate(X_XOR, y_XOR)\n",
    "print(f\"XOR Perceptron Accuracy: {xor_accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e7c415-c5db-456d-bb1d-b52c713c7793",
   "metadata": {},
   "source": [
    "Description of the code:-\n",
    "This code implements a Perceptron. The perceptron is trained to learn logical functions like NAND and XOR using supervised learning.\n",
    "\n",
    "Key Components:\n",
    "Perceptron Class\n",
    "\n",
    "Initialization (__init__): The perceptron is initialized with random weights (including a bias term), a learning rate, and a number of training epochs.\n",
    "\n",
    "Activation Function (activation): Uses a step function that outputs 1 if the weighted sum is ‚â• 0 and 0 otherwise.\n",
    "\n",
    "Prediction (predict): Computes the dot product of the input and weights, applies the activation function, and returns the predicted class (0 or 1).\n",
    "\n",
    "Training (train): Updates weights iteratively based on errors in predictions using the perceptron learning rule:\n",
    "ùë§=ùë§+learning¬†rate√óerror√óinput\n",
    "\n",
    "Evaluation (evaluate): Measures accuracy by comparing predictions with actual target values.\n",
    "\n",
    "Training on NAND Logic Gate:\n",
    "The NAND gate is a linearly separable function, meaning a perceptron can learn it.\n",
    "A perceptron is trained on this dataset, and its accuracy is calculated.\n",
    "\n",
    "Training on XOR Logic Gate:\n",
    "The XOR gate is not linearly separable, meaning a single-layer perceptron cannot learn it.\n",
    "A perceptron is trained on this dataset, but it fails to achieve good accuracy.\n",
    "\n",
    "Output:\n",
    "The perceptron successfully learn the NAND function and achieve 100% accuracy.\n",
    "The perceptron fail to learn the XOR function since XOR is not linearly separable, leading to poor accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455489b0-3601-412f-9715-e09be6ab6afd",
   "metadata": {},
   "source": [
    "Comments:-\n",
    "The Perceptron cannot solve XOR because a single-layer perceptron cannot handle non-linearly separable data.\n",
    "\n",
    "Possible Improvements:\n",
    "\n",
    "Use a Multi-Layer Perceptron (MLP) with a hidden layer.\n",
    "Implement an activation function like sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f47ee3d-065a-4513-8843-782c4bf8aa81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
